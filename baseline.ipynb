{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score\n",
    "from evaluation.evaluate_f1_partial import main as f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, splitter=\"\\t\"):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():\n",
    "                word, tag = line.split(splitter)\n",
    "                sentence.append(word)\n",
    "                tags.append(tag.strip())\n",
    "            else:\n",
    "                data.append((sentence, tags))\n",
    "                sentence = []\n",
    "                tags = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_dataset(\"data/train.tsv\")\n",
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_dataset(\"data/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "            \n",
    "for sent, tags in test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\n",
    "\n",
    "tag_to_ix = {\n",
    "    \"O\": 0,\n",
    "    \"B-Object\": 1,\n",
    "    \"I-Object\": 2,\n",
    "    \"B-Aspect\": 3,\n",
    "    \"I-Aspect\": 4,\n",
    "    \"B-Predicate\": 5,\n",
    "    \"I-Predicate\": 6\n",
    "}  # Assign each tag with a unique index\n",
    "\n",
    "idx_to_tag = dict(map(reversed, tag_to_ix.items()))\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:51<00:00,  5.83s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "for epoch in tqdm(range(50)):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also O O\n",
      ", O O\n",
      "i O O\n",
      "have O O\n",
      "recently O O\n",
      "discovered O O\n",
      "advil B-Object B-Object\n",
      "liquigels O O\n",
      "work O O\n",
      "much O O\n",
      "better B-Predicate B-Predicate\n",
      "and O O\n",
      "faster B-Predicate B-Predicate\n",
      "for O O\n",
      "a O O\n",
      "headache B-Aspect B-Aspect\n",
      "than O O\n",
      "regular O O\n",
      "ibuprofen B-Object B-Object\n",
      ". O O\n"
     ]
    }
   ],
   "source": [
    "#Вывод слова, его класса и предсказания класса\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    tags = [idx_to_tag[int(i)] for i in tag_scores.argmax(dim=-1)]\n",
    "\n",
    "    for x, y_true, y_pred in zip(training_data[0][0], training_data[0][1], tags):\n",
    "        print(x, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:00<00:00, 875.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Записываем слово, его класс и предсказание класса в файл\n",
    "with open(\"test_pred.tsv\", \"w\") as w:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_data):\n",
    "            inputs = prepare_sequence(sentence[0], word_to_ix)\n",
    "            tag_scores = model(inputs)\n",
    "            tags = [idx_to_tag[int(i)] for i in tag_scores.argmax(dim=-1)]\n",
    "            \n",
    "            for i, y in zip(sentence[0], tags):\n",
    "                w.write(f\"{i}\\t{y}\\n\")\n",
    "            w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Узнаём f1 метрику на тестовых данных\n",
    "f1_score('data/test.tsv', \"test_pred.tsv\",\"out_test_f1_10steps.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "f1_result = read_dataset(\"out_test_f1_10steps.tsv\")\n",
    "print(f1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат строгой метрики:\n",
    "f1_average_strict:\t0.422674\n",
    "f1_aspect_strict:\t0.248996\n",
    "f1_object_strict:\t0.305177\n",
    "f1_predicate_strict:\t0.776042\n",
    "\n",
    "Результат расслабленной метрики\n",
    "f1_average:\t0.450414\n",
    "f1_aspect:\t0.277108\n",
    "f1_object:\t0.336058\n",
    "f1_predicate:\t0.797179\n",
    "\n",
    "Описание результата:\n",
    "Хороший результат показывет предсказание предикатов, когда предсказание остальных сущностей показывает низкую производительность\n",
    "\n",
    "Мы будем учитывать баллы для всех отдельных классов, кроме тега O. Для объектов, состоящих из нескольких слов, мы используем “расслабленную” метрику: если границы прогнозируемого объекта совпадают с границами эталонного объекта, мы добавляем 1 к количеству TP (количество истинных положительных примеров). Если имеется только частичное совпадение, мы добавляем число от 0 до 1, вычисляемое как длина пересечения, разделенная на полную длину объекта.\n",
    "\n",
    "\"Расслабленная\" метрика как и ожидалось показывает лучшую производительность по сравнению со строгой метрикой"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
